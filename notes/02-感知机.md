# 感知机

> 这是一个线性的模型，意在用一条线（超平面）对训练数据进行二分 $\{-1, \ +1\}$，也是一个判别模型。

## 感知机模型

$${f(x) = sign(w·x + b)}$$

- 输入：训练数据的特征向量。
- 输出：二值类标。

前提是数据严格线性可分，即存在一条线（超平面）能将正负例完美分开。而学习的目的即为学出这个分离线（超平面）。

![感知机模型](http://ofqm89vhw.bkt.clouddn.com/97d8bba317a0d503afd22cb22161755a.png)

学习的策略为经验风险最小化，即误分类点数最少，而假设是线性可分，因此误分类点数一定可以降为 ${0}$。但是，误分类点数并不能指导我们如何修改模型（不能导出有效的算法），因此我们修改了一下策略的表示，改为误分类点到分离平面的距离之和。因此损失函数为：

$${L(w, b) = -\sum\limits_{x_i\in M} y_i(w_i+b)}$$

其中，${(M)}$为误分类点集，${(w)}$ 和 ${(b)}$ 为参数，${(y_i)}$ 是标准类标，乘上 ${(y_i)}$ 乘积保证为正。为求其极小，分别对参数求导并令其导数为 ${0}$ 即可：

$${\nabla_w L(w, b) = -\sum_{x_i\in M}y_i x_i}$$

$${\nabla_b L(w, b) = -\sum_{x_i\in M}y_i}$$

学习算法为梯度下降法，有原始形式和对偶形式之分。

原始形式核心递归式：

## 感知机学习策略

## 本章概要

1. 感知机是根据输入实例的特征向量 ${x}$ 对其进行二分类的线性分析模型：${f(x) = sign(w·x + b)}$，感知机对应输入空间（特征空间）中的分离超平面 ${w·x + b = 0}$。
1. 感知机学习的策略是极小化损失函数：${\min_{w, b} L(w,b) = - \sum_{x_i \in M} y_i (w·x_i + b)}$，损失函数对应五分类点到分离超平面的总距离。
1. 感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式。算法简单且易于实现。原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。在这个过程中一次随机选取一个误分类点使其梯度下降。
1. 当训练数据集线性可分时，感知机学习算法是手链的。感知机算法在训练数据集上的误分类次数 ${k}$ 满足不等式：${k \le \Big( \frac{R}{\gamma} \Big)^2}$，当训练集数据线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不用的迭代顺序而可能有所不同。
