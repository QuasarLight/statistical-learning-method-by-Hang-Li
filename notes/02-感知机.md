# 感知机

> 感知机是一个线性的模型，意在用一条线（超平面）对训练数据进行二分 $\{-1, \ +1\}$，也是一个判别模型。

## 感知机模型

$${f(x) = sign(w·x + b)}$$

- 输入：训练数据的特征向量。
- 输出：二值类标。

前提是数据严格线性可分，即存在一条线（超平面）能将正负例完美分开。而学习的目的即为学出这个分离线（超平面）。

![感知机模型](http://ofqm89vhw.bkt.clouddn.com/97d8bba317a0d503afd22cb22161755a.png)

学习的策略为经验风险最小化，即误分类点数最少，而假设是线性可分，因此误分类点数一定可以降为 ${0}$。但是，误分类点数并不能指导我们如何修改模型（不能导出有效的算法），因此我们修改了一下策略的表示，改为误分类点到分离平面的距离之和。因此损失函数为：

$${L(w, b) = -\sum\limits_{x_i\in M} y_i(w_i+b)}$$

其中，${(M)}$为误分类点集，${(w)}$ 和 ${(b)}$ 为参数，${(y_i)}$ 是标准类标，乘上 ${(y_i)}$ 乘积保证为正。为求其极小，分别对参数求导并令其导数为 ${0}$ 即可：

$${\nabla_w L(w, b) = -\sum_{x_i\in M}y_i x_i}$$

$${\nabla_b L(w, b) = -\sum_{x_i\in M}y_i}$$

学习算法为梯度下降法，有原始形式和对偶形式之分。

原始形式核心递归式：

## 感知机学习策略

给定一个线性可分的数据集

$${T = \{ (x_1,y_1),(x_2,y_2),...(x_N,y_N)\}}$$

其中 ${x_i \in X=R^n}$ ，${y_i \in Y = \{ +1, -1\}, i = 1,2,3,...N}$。

为了确定感知机模型的参数 ${w}$ 和 ${b}$，需要确定一个学习策略，即定义一个损失函数并将损失函数极小化。感知机采用的损失函数为误分类点到超平面的总距离。首先写出输入空间 ${R^n}$ 中任一点 ${x_0}$ 到分离超平面的距离

$${\frac{1}{\|w\|}|w \cdot x_0 + b|}$$

这里 ${\|w\|}$ 是 ${w}$ 的 ${L2}$ 范数。

其次对于误分类的数据 ${(x_i, y_i)}$ 来说，

$${−y_i(w \cdot x_i + b)>0}$$

因为当w⋅xi+b>0，yi=−1，而当w⋅xi+b<0，yi=+1。因此误分类点xi到超平面的距离是
−1‖w‖yi(w⋅xi+b)
这样假设误分类点的集合为M，那么所有误分类点到超平面的总距离为
−1‖w‖∑xi∈Myi(w⋅xi+b)
不考虑1‖w‖，就得到感知机学习的损失函数
L(w,b)=−∑xi∈Myi(w⋅xi+b)
显然，损失函数 ${L(w,b)}$ 是非负的。如果没有误分类点，损失函数值为 ${0}$，而且，误分类点越少，误分类点离超平面越近，损失函数的值越小。
感知机学习的策略是在假设空间中选取使损失函数最小的模型参数 ${w}$ , ${b}$。

## 本章概要

1. 感知机是根据输入实例的特征向量 ${x}$ 对其进行二分类的线性分析模型：${f(x) = sign(w·x + b)}$，感知机对应输入空间（特征空间）中的分离超平面 ${w·x + b = 0}$。
2. 感知机学习的策略是极小化损失函数：${\min_{w, b} L(w,b) = - \sum_{x_i \in M} y_i (w·x_i + b)}$，损失函数对应五分类点到分离超平面的总距离。
3. 感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式。算法简单且易于实现。原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。在这个过程中一次随机选取一个误分类点使其梯度下降。
4. 当训练数据集线性可分时，感知机学习算法是手链的。感知机算法在训练数据集上的误分类次数 ${k}$ 满足不等式：${k \le \Big( \frac{R}{\gamma} \Big)^2}$，当训练集数据线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不用的迭代顺序而可能有所不同。

## 习题

> Minsky 与 Papert 指出：感知机因为是线性模型，所以不能表示复杂的函数，如亦或 ${(XOR)}$，验证感知机为什么不能表示异或。

> 模仿题录