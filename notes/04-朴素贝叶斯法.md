# 朴素贝叶斯法

> 朴素贝叶斯（Naive Bayes）算法是机器学习中常见的基本算法之一，它主要被用来做分类任务。其理论基础是基于贝叶斯定理和条件独立性假设的一种分类方法。

![](http://ofqm89vhw.bkt.clouddn.com/9a1a0c1e1b72dc54996c447f2ee34423.png)

## 基本方法

![](http://ofqm89vhw.bkt.clouddn.com/be0eaabeb5d729b7dfdf4745f2a7efc1.png)

朴素贝叶斯法对条件概率分布做了条件独立的假设，是一个较强的假设。

![条件独立假设](http://ofqm89vhw.bkt.clouddn.com/b8833717199512a73d9e39f60f4af353.png)

![公式推导](http://ofqm89vhw.bkt.clouddn.com/26f5894071c57e83e72297a784d8455f.png)

### 最大后验概率的含义

等价于期望风险最小化。

![](http://ofqm89vhw.bkt.clouddn.com/4d402ee0bd8f22472b12a3e512e5ef64.png)

## 朴素贝叶斯法的学习与分类

## 朴素贝叶斯法的参数估计

### 极大似然估计

![](http://ofqm89vhw.bkt.clouddn.com/1def46d88b94835b7919f39ca56355dd.png)

### 学习与分类算法

输入：训练数据 ${T = \{(x_1, y_1), (x_2, y_2), \cdots (x_n, y_n)}\}$，其中，${x_i = (x_i^{(1)}, x_i^{(2)}, \cdot, x_i^{(n)})^T}$，${x_i^{(j)}}$ 是第 ${i}$ 个样本的第 ${j}$ 个特征，

输出：实例 ${x}$ 的分类。

计算先验概率及条件概率。
对于给定的实例，计算。
确定实例 ${x}$ 的类。

### 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为 ${0}$ 的情况，这回影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。

通常取得 ${\lambda = 1}$，这时成为拉普拉斯平滑。

![](http://ofqm89vhw.bkt.clouddn.com/0fd281538f1c05a55549ecd66a6af84d.png)

### 本章概要

1. 朴素贝叶斯是典型的生成学习方法。生成方法由训练数据学习联合概率分布 ${P(x, y)}$，然后求得后验概率分布 ${P(Y|X)}$ ，具体地来说，利用训练数据学习 ${P(X|Y)}$ 和 ${P(Y)}$ 的估计，得到联合概率分布：${P(X, Y) = P(Y) P(X|Y)}$，概率估计方法可以是极大似然估计或贝叶斯估计。
1. 朴素贝叶斯的基本假设是条件独立性，这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大卫简化。因为朴素贝叶斯法高效，且易于实现，缺点是分类的性能不一定很高。
1. 朴素贝叶斯法利用贝叶斯定理与学到的联合概率分布进行分类预测，${P(Y|X) = \frac{P(X, Y)}{P(X)}}$，将输入 ${x}$ 分到后验概率最大的类 ${y}$，${y = \arg \max_{c_k} P(Y = c_k)}$，后验概率最大等价于 ${0-1}$ 损失函数期望风险最小化。

### 习题