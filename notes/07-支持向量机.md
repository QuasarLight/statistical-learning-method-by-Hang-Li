# 支持向量机

## 函数间隔和几何间隔

一个点距离分类超平面的远近可以表示分类预测的确信程度。

用 ${y(w·x + b)}$ 来表示分类的正确性及确信度，这就是函数间隔的概念。

对分离超平面的法向量 ${w}$ 加些约束，如规范化，${||w||=1}$, 使得间隔是确定的，这是函数间隔成为了几何间隔。

$${\gamma_i = y_i \Big( \frac{w}{||w||} · x_i + \frac{b}{||w||} \Big) }$$

## 间隔最大化

对训练数据找到几何间隔最大化的超平面意味着以充分大的确信度对训练数据进行分类。

## 学习的对偶算法

通过求解对偶问题得到原始你问题的最优解，这就是线性可分支持向量机的对偶算法。这样做的优点，意识对偶问题往往更容易求解；二是自然引入核函数，今儿推广到非线性分类问题。

## 合页损失函数

## 正定核

## 序列最小最优化算法

### 两个变量二次规划的求解方法

## 非线性支持向量机与核函数

对于解线性分类问题，线性分类支持向量机是一种非常有效的方法，但是当面对分类问题是非线性的，这时可以使用核技巧（knrnel trick）。

### 核技巧

![非线性分类问题与核技巧示例](http://ofqm89vhw.bkt.clouddn.com/8c6c0557585f65ea9b016f719a914c63.png)

### 核函数的定义

核技巧的想法是，在学习与预测中只定义核函数 ${K(x,z)}$，而不显示地定义映射函数 ${\Phi}$。通常，直接计算 ${K(x, z)}$ 比较容易，而通过 ${\Phi(x)}$ 和 ${\Phi(z)}$ 计算 ${K(x,z)}$ 并不容易。

### 核技巧在支持向量机中的应用

在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的`内积`。在对偶问题的目标函数中的内积 ${x_i · x_j}$ 可以用核函数 ${K(x_i, x_j) = \Phi(x_i) · \Phi(x_j)}$ 来代替，所以，对偶问题的目标函数化简成为：

$${W(\alpha) = \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) - \sum_{i=1}^{N} \alpha_i}$$

## 正定核

