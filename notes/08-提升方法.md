# 提升方法

> 提升（boosting）方法是一种常见的统计学习方法，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

## 提升方法 ${AdaBoost}$ 算法

### 提升方法的基本思路

对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

在概率近似正确（probably approximately correct，PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。

在学习中，如果已经发现了“弱学习算法”
![由弱到强](http://ofqm89vhw.bkt.clouddn.com/3c3ae3384b76caa1f2df1c5ccea5ebf0.png)

![${AdaBoost}$](http://ofqm89vhw.bkt.clouddn.com/7ec4802e108a40440695fb97663d069b.png)

大多数的提升方法都是通过改变数据的概率分布（训练数据的权值分布），针对不同的训练数据调用弱学习算法学习一系列弱分类器，这样针对boosting需要解决两个问题：

1. 如何改变训练数据的权值或概率分布。
1. 如何将弱分类器组合为一个强分类器。

### ${AdaBoost}$ 算法

`输入`：训练数据集 ${T={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}}$ ,其中 ${x_i \in R^n}$ , ${y_i \in \{-1, +1\}}$;某个弱学习算法 ${g(x)}$。

`输出`：最终的分类器 ${G(x)}$。

1. 初始化训练数据的权值 ${D_1=\{w_{11},...,w_{1i},...w_{1N}\}}$，${w_{1i}= \frac{1}{N}}$。

a) 使用具有权值分布 ${D_m}$ 的训练数据集学习，得到基本分类器 ${g_m(x)}$；

b) 计算 ${g_m(x)}$ 在训练数据上的分类误差率，${e_m=P(g_m(xi)\neq y_i)=\sum_{i=1}^{N}w_{mi} I(g_m(xi)\neq y_i)}$;

c) 计算 ${g_m(x)}$ 的系数，${\alpha_m= \frac{1}{2} \log^{\frac{1 - e_m}{e_m}}}$ (${\alpha_m}$ 随着 ${e_m}$ 的减小而增大，说明分类误差率小的分类器在最终的分类器占据的权重较大);

d) 更新训练数据的权值分布，${w_{(m+1)i} = \frac{w_{mi}}{Z_m}\exp(-\alpha y_i g_m(x_i))}$，其中 ${Z_m=\sum_{i=1}^{N}w_{mi} \exp(-\alpha y_i g_m(xi))}$，是规范化因子，目的是使得 ${D_{m+1}}$成为一个概率分布；（当分类错误时，${-\alpha y_i g_m(x_i)=\alpha}$;当分类争正确时，${-\alpha y_i g_m(x_i)=-\alpha}$，故错误的分类样本权值将增大，正确分类样本的权值将减小）

1. 通过基本分类器的线性组合获得最终的分类器，${G(x)=sign(\sum_{i=1}^{N} \alpha_m g_m(x))}$

## ${AdaBoost}$ 算法的训练误差

${AdaBoost}$ 最终分类器的训练误差界为：

$${\frac{1}{N} \sum_{i=1}^{N} I(G(x_i) \neq y_i) \leq \frac{1}{N} \sum_{i} \exp(-y_i f(x_i)) = \prod_m Z_m}$$

## 提升树

> 提升树是一分类树或回归树为基本分类器的提升方法。被认为是统计学习中最好的方法之一。

### 提升树模型

提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。

$${f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)}$$

其中，${T(x;\Theta_m)}$ 表示决策树，${\Theta_m}$ 为决策树的参数，${{M}}$ 为树的个数。

### 提升树算法

### 梯度提升