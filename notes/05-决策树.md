# 决策树

决策树是一种基本的分类与回归方法，决策树呈树形结构，在分类问题找那个，表示基于特征对实例进行分类的过程。它可以认为是 ${if-then}$ 规则的集合。也可以认为是定义在特征空间与类空间上的条件概率分布，主要优点是模型具有可读性，方便可视化，分类速度快，模型在学习的过程中，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括 ${3}$ 个步骤：特征选择，决策树的生成和决策树的修剪。

## 决策树模型与学习

### 决策树模型

决策树模型是一种描述对实例进行分类的树形结构。一旦决策树构建完成，那么利用决策树进行分类预测是非常简单的：首先，从根节点开始，对实例的某一特征进行对比测试，根据测试结果将该实例分配到相应的子节点上；这时，每一个子节点对应着该特征的一个取值（或取值范围）。如此递归地对实例进行测试，直至该实例被分配到叶子节点，最后将实例分配到叶子节点所代表的类中。

![决策树对应特征空间划分和条件概率分布](http://ofqm89vhw.bkt.clouddn.com/15a2483c23c9f53b4f9a6de0541cd09e.png)

对于决策树中的非叶子节点，我们可以将其看成是一个规则的条件；叶子节点可以看成是规则的最终结论；而决策树中的边，我们可以将其理解成规则划分的过程。决策树的路径或其对应的 IF-THEN 规则集合具有一个十分重要的性质：互斥且完备。也就是说，每一个样本空间上的实例都可以被一条或一条规则链所覆盖，而且只被一条路径或一条规则链所覆盖。

## 特征选择

### 特征选择问题

通常特征选择的准则是信息增益或者信息增益比。

理解为不断地提高所划分部分的纯度，即：使得各个子集在当前条件下有最好的分类，那么久应该选择这个特征。

所谓的特征选择其实是分两个步骤的：

1. 首先，需要选择对训练数据具有最大分类能力的特征进行树的叶子节点的分裂；
1. 然后，选择该特征合适的分裂点进行分裂。

### 信息增益

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。

假设 ${X}$ 是一个具有有限个值的离散型随机变量，服从如下的概率分布：

$${P(X = x_i) = p_i, \ \ \ \ i = 1, 2, \cdots, n}$$

那么，随机变量 ${X}$ 的熵定义为：

$${H(X) = - \sum_{i=1}^{n} p_i \log^{p_i}}$$

> 举例：分布为贝努利分布时熵与概率的关系

![分布为贝努利分布时熵与概率的关系](http://ofqm89vhw.bkt.clouddn.com/7b5061af69f575ec2b06ff369b0e269c.png)

信息增益（information gain）表示得知特征 ${X}$ 的信息而使得类 ${Y}$ 的信息不确定性减少的程度。

设随机变量 ${(X,Y)}$ 的联合概率分布为：

$${P(X=x_i, Y=y_j) = p_{ij}, \ i = 1, 2, \cdots, m}$$

条件熵 ${H(Y|X)}$ 表示在已知随机变量 ${X}$ 的取值条件下随机变量 ${Y}$ 的不确定性，其数学定义为 ${X}$ 给定的条件下 ${Y}$ 的条件概率分布的熵对 ${X}$ 的数学期望，数学表达式如下：

$${H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)}$$

$${g(D,A) = H(D) - H(D|A)}$$

## 决策树的生成

### ${ID3}$ 算法

### ${C4.5}$ 的生成算法

## 决策树的剪枝

## ${CART}$ 算法

### ${CART}$ 剪枝

## 本章概要

## 习题